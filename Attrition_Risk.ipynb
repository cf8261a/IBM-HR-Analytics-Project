{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d6423e",
   "metadata": {},
   "source": [
    "# **Attrition Risk Modeling**\n",
    "<br/>by Christian Fernandes\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ed3ab",
   "metadata": {},
   "source": [
    "To leave or not to leave? That is the question many of us in the workforce have probably at least once in our careers considered. Whether it be because we want to chase that next job title or improve our work-life balance there are many reasons why we'd choose to jump ship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0260e9",
   "metadata": {},
   "source": [
    "Well, what does this mean for the employer? In terms of the bottom line, high attrition (voluntary terminations) could engender a number of issues that include but are certainly not limited to higher costs of replacing employees, decreased workplace morale, and potentially, a snowball effect that leads to more and more turnovers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56f5ac",
   "metadata": {},
   "source": [
    "What can employers do? They can take advantage of employee data that is tracked in HR systems like Workday and partition it into rosters that capture meaningful employee information such as their job title, current manager, line of business hierarchy, manager hierarchy, salary history, etc. \n",
    "\n",
    "Here are some things to consider:\n",
    "<ul>\n",
    "    <li> Using geospatial information like addresses, we can pair this data with employee HQ location data to determine the commuting distance for employees. If we pair this with a flag that identifies remote versus in-office employees then we can see how turnover effects the former versus the latter. </li>\n",
    "    <li> Using employee start dates and the current date, we could determine the length of service for a given employee and see if this could serve as a proxy for a lack of upward mobility. </li>\n",
    "    <li> Using age information, could it be the case that younger employees are more prone to job hop in today's market? For employees at retirement age, what does turnover look like for this group?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e387ef",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68327cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7015493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5196c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "empattr = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284e9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "empattr.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "empattr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b82990",
   "metadata": {},
   "source": [
    "The dataset has 35 variables and 1470 observations. Several things we need to be aware of: \n",
    "<ul> \n",
    "    <li> Small sample sizes in general can lead to instability in our model parameter estimates </li>\n",
    "    <li> Ideally, our response variable should be an i.i.d random variable otherwise, any predictions made from this data could be way off.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf777be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "empattr.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any values that are null?\n",
    "if all(empattr.isnull().any() == False): \n",
    "    print(\"Dataset does not have any null values\")\n",
    "else:\n",
    "    print(\"There are null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns that are considered of object type. \n",
    "objcols = [c for c in empattr.columns if empattr[c].dtype == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataset description, these are factor variables\n",
    "catcols = [\"Education\", \"EnvironmentSatisfaction\", \"JobInvolvement\", \"JobSatisfaction\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"WorkLifeBalance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051ab2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "objcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if any of the object type columns are in the categorical column list defined in the IBM metadata\n",
    "# Result is an empty set\n",
    "set(objcols).intersection(set(catcols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69115c1",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(tibble, colname, response_var):\n",
    "    tibble[[colname, response_var]].boxplot(by = response_var, grid = False)\n",
    "\n",
    "def plot_barchart(series_col):\n",
    "    \"\"\"a is pd.Series object that names the column to be converted to a barchart\"\"\"\n",
    "    counterDict = dict(Counter(series_col))\n",
    "    df = pd.DataFrame({'labels': counterDict.keys(), 'values': counterDict.values()})\n",
    "\n",
    "    df.plot.bar(x = 'labels', y = 'values', title = series_col.name)\n",
    "    \n",
    "def plot_figure(tibble, colname, xlab = None, ylab = None, fig_type = None, response_var = None):\n",
    "    from collections import Counter\n",
    "    \n",
    "    if fig_type:\n",
    "        if fig_type == 'hist':\n",
    "            tibble[colname].hist(grid = False)\n",
    "        elif fig_type == 'boxplot':\n",
    "            plot_hist(tibble, colname, response_var)\n",
    "        elif fig_type == 'barh':\n",
    "            plot_barchart(tibble[colname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for the categorical data\n",
    "\n",
    "# First ensure all columns are of the 'object' type - similar to R's as.factor() command\n",
    "empattr_obj = empattr[objcols + catcols].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "empattr_obj.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70a5cb",
   "metadata": {},
   "source": [
    "Already, we can see for some of the observations that a majority of the employees in this dataset did not quit their jobs (Attrition = No). We can also see that a large proportion of observations are male employees (they make up a majority relative to the other gender 'female').   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde18663",
   "metadata": {},
   "outputs": [],
   "source": [
    "printcols = ['Attrition', 'Gender', 'PerformanceRating']\n",
    "for c in printcols:\n",
    "    plot_figure(empattr, c,  xlab = c, ylab = None, fig_type = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8714903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for the numerical data\n",
    "\n",
    "# Grabbing all of the columns that were not identified as categorical (not in the empattr_obj dataframe)\n",
    "empattr_numerical_cols = list(set(empattr.columns.tolist()).difference(set(objcols + catcols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0650d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "empattr[empattr_numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize = (10, 6))\n",
    "plt.subplot(1,3,1)\n",
    "plot_figure(empattr, 'Age', 'Age', None, 'hist')\n",
    "plt.subplot(1,3,2)\n",
    "plot_figure(empattr, 'YearsSinceLastPromotion', 'YearsSinceLastPromotion', None, 'hist')\n",
    "plt.subplot(1,3,3)\n",
    "plot_figure(empattr, 'DistanceFromHome', 'DistanceFromHome', None, 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(empattr, 'Age', fig_type = 'boxplot', response_var = 'Attrition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0ff74",
   "metadata": {},
   "source": [
    "In the age boxplot, it appears that the center of the distribution (as well as the interquartile range) for those who did leave the company seems to hover closer towards those in the late 20s and late 30s age range. \n",
    "\n",
    "For those bucketed in the 'No' plot, the retention with respect to age seems to be centered closer towards mid 30s with the interquartile window further up between early 30s and early 40s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(empattr, 'YearsSinceLastPromotion', fig_type = 'boxplot', response_var = 'Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(empattr, 'DistanceFromHome', fig_type = 'boxplot', response_var = 'Attrition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37339807",
   "metadata": {},
   "source": [
    "Boxplots allow us to see the center and spread of our data across groups. In the graphs above, we looked at the relationship of the response, Attrition, with the three predictors of interest to see if any appear to have a noticeable relationship. Age and DistanceFromHome both seem to have slightly different centers and roughly proportional spread (possible relationship?). \n",
    "\n",
    "We have to keep in mind, though, that these plots show us the individual bivariate relationship between attrition and each predictor. This implies that if we are building out a model to either describe, explain, or predict attrition in the workplace, we need to be cognizant of the conditional relationship between two or more variables with respect to our response (attrition). For example, an a multiple regression setting (whether it be linear, poisson, logistic, etc.), multicollinearity or near-multicollinearity among predictors say age and years since last promotion could impact the sign, size, and significance of our estimates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0094f",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a645bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the correlation among variables (not including EmployeeCount, Standard Hours, or non-numerical columns)\n",
    "ignore_cols = ['EmployeeCount','StandardHours'] + objcols + catcols\n",
    "empcor = empattr.drop(columns =ignore_cols).corr()\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(empcor, annot=True, fmt = '.0%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fff4b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89ce53",
   "metadata": {},
   "source": [
    "Let's suppose that the variables age, YearsSinceLastPromotion, and DistanceFromHome would all provide predictive power  for whether or not an employee will leave the company. \n",
    "\n",
    "We can run a logistic regression model using these three variables and assess if the model provides accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b1a9c",
   "metadata": {},
   "source": [
    "In Logistic Regression, we assume three things, \n",
    "<br/>\n",
    "<ul>\n",
    "  <li>The logit of our response variable (the log odds) are a linear combination of our predictors and their coefficients</li>\n",
    "  <li>Our responses are independent random variables</li>\n",
    "  <li>Our link function is the logit function (the log odds) since it is possible to use other link functions like the probit function etc.</li>\n",
    "</ul>\n",
    "\n",
    "### Submodel with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelformula = 'Attrition ~ Age + YearsSinceLastPromotion + DistanceFromHome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774418df",
   "metadata": {},
   "outputs": [],
   "source": [
    "smf_model = smf.glm(formula = modelformula, data=empattr, family=sm.families.Binomial())\n",
    "result = smf_model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab58eb",
   "metadata": {},
   "source": [
    "Given an $\\alpha$ threshold of 0.05, we can see that all the p-values for the three chosen predictors are indeed less than the threshold and therefore statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550afa6",
   "metadata": {},
   "source": [
    "## Divide up the data intro training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-map the Yes's to 1s and No's to 0s\n",
    "# response = empattr['Attrition'].apply(lambda x: 1 if x.upper() == 'YES' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = empattr[['Age', 'YearsSinceLastPromotion', 'DistanceFromHome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = empattr[['Attrition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(predictors, response, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the training predictor variables back with the training response variables (because out smf.glm() model \n",
    "# will be referencing both)\n",
    "xtrain2 = xtrain.merge(ytrain, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14900e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smf_model_train = smf.glm(formula = modelformula, data=xtrain2, family=sm.families.Binomial())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4989453",
   "metadata": {},
   "outputs": [],
   "source": [
    "smf_model_train_result = smf_model_train.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd19359",
   "metadata": {},
   "outputs": [],
   "source": [
    "smf_model_train_result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b8504",
   "metadata": {},
   "source": [
    "### Testing for overall regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be369b9",
   "metadata": {},
   "source": [
    "By taking the difference between our null deviance and deviance statistic, we are able to assess whether not the model itself is statistically significant.\n",
    "\n",
    "Recall that in logistic regression, we maximize the log-likelihood function under the model with no parameters (so intercept only) and we maximize the log-likelihood function under the model with all of the predictors of interest (so Age, DistanceFromHome, and YearsSinceLastPromotion). \n",
    "\n",
    "We then take the difference of the two models (so the intercept only model would just be the null deviance) and calculate their differences. This yields a $\\chi^2$ statistic with degrees of freedom equal to the number of predictors in our model: 3.\n",
    "\n",
    "Like the F-test for a multiple linear regression model, the null hypothesis is that all of the coefficient estimates in our full model are 0 versus the alternative hypothesis that **atleast 1** is non-zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming large data\n",
    "overallpvalue = chi2.sf(smf_model_train_result.null_deviance - smf_model_train_result.deviance, 3)\n",
    "\n",
    "print(f\"\"\"Our p-value is {overallpvalue}. \\n \n",
    "This is much lower than the typical alpha threshold of 0.05. At this level, our model is statiscally significant \n",
    "because atleast one of the predictors included in the model has explanatory power with respect to Attrition\"\"\")\n",
    "# Alternative way to comput the p(chi^2 > smf_model_train_result.null_deviance - smf_model_train_result.deviance)\n",
    "# 1-chi2.cdf(smf_model_train_result.null_deviance - smf_model_train_result.deviance, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the fitted values from the training set\n",
    "fitted_xtrain2 = smf_model_train_result.fittedvalues\n",
    "\n",
    "# Recoding the results (probabilities) into 0 for No and 1 for Yes\n",
    "fitted_xtrain2_recoded = fitted_xtrain2.apply(lambda x: 0 if x > 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877adf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a quick look at the number of Nos (0) and Yes (1)\n",
    "Counter(ytrain['Attrition'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6430a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The # of No's and Yes's in the fitted_xtrain_recoded set given our threshold above which was 0.5\n",
    "Counter(fitted_xtrain2_recoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd495f25",
   "metadata": {},
   "source": [
    "The model with just the three predictors - age, distance from home, and years since last promotion- predicted all of the values to be \"no\". In other words, it predicted 988/(988+188) or 84% of the values correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9246bc",
   "metadata": {},
   "source": [
    "### Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = smf_model_train_result.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a threshold \n",
    "\n",
    "# Assuming 50/50 chance\n",
    "\n",
    "th = 0.5\n",
    "\n",
    "predictions_results = predictions_test.apply(lambda x: 0 if x > th else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(predictions_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(ytest['Attrition'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ac4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_results_recoded = predictions_results.apply(lambda x: 'No' if x == 0 else 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c393c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(predictions_results_recoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00e345",
   "metadata": {},
   "source": [
    "Our model predicted all of the values to be 'No' in this situation (similar to its behavior with our training set). Effectively, that means that our predictions were 83% accurate. In machine learning, we would expect to see similar results as we move from our training sample to our testing sample when we are working with a model with a few predictors. As we add more predictor variables to the dataset, we might end up with a more accurate result during the training phase but suffer from poor performance in the testing phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a953a4d",
   "metadata": {},
   "source": [
    "## Can we do better? - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646679d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Use a random forest classifer, lets start with 50 trees\n",
    "attr_forest = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "\n",
    "attr_forest.fit(xtrain, ytrain.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e68b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the accuracy of this model?\n",
    "attr_forest.score(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_forest_CM = confusion_matrix(ytest, attr_forest.predict(xtest))\n",
    "display(attr_forest_CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5632053",
   "metadata": {},
   "source": [
    "How to read the confusion matrix: \n",
    "<table align = \"left\">\n",
    "  <tr>\n",
    "    <td>TN</td>\n",
    "    <td>FP</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>FN</td>\n",
    "    <td>TP</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "<ul>\n",
    "    <li>The True Positive (TP) value is at position 1,1</li>\n",
    "<li>The True Negative (TN) value is at position 0,0</li>\n",
    "<li>The False Negative (FN) value is at position 1,0</li>\n",
    "<li>The False Positive (FP) value is at position 0,1</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ea499",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = attr_forest_CM[1,1]\n",
    "tn = attr_forest_CM[0,0]\n",
    "\n",
    "fm_test_acc = (tp + tn)/xtest.shape[0]\n",
    "print(f\"The accuracy of the random forest on the test set is {fm_test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596727b",
   "metadata": {},
   "source": [
    "### Logistic Regression with sklearn and almost full predictor set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c4c11",
   "metadata": {},
   "source": [
    "This time we'll look at most of the columns. Recall that EmployeeCount and StandardHours were constants (single level factors that don't really contribute to understanding Attrition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = empattr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.drop(columns = ['EmployeeCount','StandardHours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ced9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "recodecols = catcols + objcols\n",
    "for c in recodecols:\n",
    "    newdf[c] = LabelEncoder().fit_transform(newdf[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newPreds = newdf.iloc[:, newdf.columns != 'Attrition']\n",
    "newY = newdf['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(newPreds, newY, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model using an L1 penalty and Coordinate Descent optimizer\n",
    "newmodel = LogisticRegression(penalty = 'l1', solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel.score(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "preds = newmodel.predict(xtest) == ytest\n",
    "recodedpreds = preds.apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(recodedpreds)/len(recodedpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e65642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
